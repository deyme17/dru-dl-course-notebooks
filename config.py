import torch

def check_environment():
    """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è"""
    print("=" * 60)
    print("–Ü–ù–§–û–†–ú–ê–¶–Ü–Ø –ü–†–û –°–ï–†–ï–î–û–í–ò–©–ï")
    print("=" * 60)

    # PyTorch –≤–µ—Ä—Å—ñ—è
    print(f"üì¶ PyTorch –≤–µ—Ä—Å—ñ—è: {torch.__version__}")

    # CUDA –¥–æ—Å—Ç—É–ø–Ω—ñ—Å—Ç—å
    cuda_available = torch.cuda.is_available()
    print(f"üñ•Ô∏è  CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {cuda_available}")

    if cuda_available:
        print(f"üîß CUDA –≤–µ—Ä—Å—ñ—è: {torch.version.cuda}")
        print(f"üìä –ö—ñ–ª—å–∫—ñ—Å—Ç—å GPU: {torch.cuda.device_count()}")

        for i in range(torch.cuda.device_count()):
            print(f"\nGPU {i}:")
            print(f"  –ù–∞–∑–≤–∞: {torch.cuda.get_device_name(i)}")
            props = torch.cuda.get_device_properties(i)
            print(f"  –ü–∞–º'—è—Ç—å: {props.total_memory / 1024 ** 3:.2f} GB")
            print(f"  Compute Capability: {props.major}.{props.minor}")
    else:
        print("‚ö†Ô∏è  GPU –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ CPU.")
        print("–î–ª—è –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó GPU –≤ Colab: Runtime -> Change runtime type -> GPU")

    # CPU —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è
    print(f"\nüíª CPU threads: {torch.get_num_threads()}")

    return cuda_available


# –ó–∞–ø—É—Å–∫ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏
cuda_available = check_environment()
device = torch.device("cuda" if cuda_available else "cpu")
print(f"\n‚úÖ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ device: {device}\n")
